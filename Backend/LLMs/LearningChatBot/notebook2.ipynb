{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"Groq API key not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\importlib\\__init__.py:90: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  return _bootstrap._gcd_import(name[level:], package, level)\n",
      "C:\\Users\\zubay\\AppData\\Local\\Temp\\ipykernel_4104\\2605817715.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\zubay\\.conda\\envs\\fyp\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the HuggingFace Embedding Model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from a text file and add metadata\n",
    "def load_documents(file_path):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "    \n",
    "    # Add metadata to each document (e.g., file name)\n",
    "    for doc in documents:\n",
    "        doc.metadata[\"source\"] = file_path\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your subtopic and chapter separators\n",
    "\n",
    "# Custom RecursiveCharacterTextSplitter with regex patterns for subtopics and chapters\n",
    "class CustomTextSplitter(RecursiveCharacterTextSplitter):\n",
    "    def __init__(self, **kwargs):\n",
    "        subtopic_pattern = re.compile(r'(\\d+(\\.\\d+)+)')\n",
    "        chapter_separator = 'chapter end -------------------------------------'\n",
    "\n",
    "        # Initialize with any other parameters, and add your separators\n",
    "        super().__init__(separators=[chapter_separator], **kwargs)\n",
    "        self.subtopic_pattern = subtopic_pattern\n",
    "\n",
    "    def split_text(self, text):\n",
    "        # First, split by chapters\n",
    "        texts = super().split_text(text)\n",
    "        documents = []\n",
    "        \n",
    "        # For each chapter, split by subtopic using the subtopic regex\n",
    "        chapter_number = 1\n",
    "        for chapter in texts:\n",
    "            subtopic_splits = self._split_by_subtopic(chapter, chapter_number)\n",
    "            documents.extend(subtopic_splits)\n",
    "            chapter_number += 1\n",
    "        \n",
    "        return documents\n",
    "\n",
    "    def _split_by_subtopic(self, text, chapter_number):\n",
    "        # Use the subtopic regex to split text\n",
    "        matches = list(self.subtopic_pattern.finditer(text))\n",
    "        if not matches:\n",
    "            # No subtopics found, return the full text as a single Document\n",
    "            return [Document(page_content=text.strip(), metadata={\"chapter\": chapter_number})]\n",
    "        \n",
    "        subtopics = []\n",
    "        start_idx = 0\n",
    "        subtopic_number = 1\n",
    "        \n",
    "        for match in matches:\n",
    "            end_idx = match.start()\n",
    "            if start_idx != end_idx:\n",
    "                subtopics.append(Document(\n",
    "                    page_content=text[start_idx:end_idx].strip(),\n",
    "                    metadata={\"chapter\": chapter_number, \"subtopic\": subtopic_number}\n",
    "                ))\n",
    "            start_idx = end_idx\n",
    "            subtopic_number += 1\n",
    "            \n",
    "        # Append the remaining part as a subtopic\n",
    "        subtopics.append(Document(\n",
    "            page_content=text[start_idx:].strip(),\n",
    "            metadata={\"chapter\": chapter_number, \"subtopic\": subtopic_number}\n",
    "        ))\n",
    "        \n",
    "        return subtopics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and handle storage\n",
    "def embed_documents(split_docs, embedding_model):\n",
    "    EMBEDDINGS_FOLDER = \"embeddings\"\n",
    "    EMBEDDINGS_FILE = os.path.join(EMBEDDINGS_FOLDER, \"emb01.pkl\")\n",
    "\n",
    "    if not os.path.exists(EMBEDDINGS_FOLDER):\n",
    "        os.makedirs(EMBEDDINGS_FOLDER)\n",
    "\n",
    "    if os.path.exists(EMBEDDINGS_FILE):\n",
    "        print(f\"Loading existing embeddings from {EMBEDDINGS_FILE}...\")\n",
    "        with open(EMBEDDINGS_FILE, 'rb') as f:\n",
    "            embedded_docs = pickle.load(f)\n",
    "    else:\n",
    "        print(\"Creating new embeddings...\")\n",
    "        texts = [doc.page_content for doc in split_docs]\n",
    "        embedded_docs = embedding_model.embed_documents(texts)\n",
    "\n",
    "        with open(EMBEDDINGS_FILE, 'wb') as f:\n",
    "            pickle.dump(embedded_docs, f)\n",
    "\n",
    "    return embedded_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store embeddings in Chroma vector store\n",
    "def store_embeddings(split_docs, embedding_model):\n",
    "    vector_store = Chroma.from_documents(split_docs, embedding_model) \n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_rag_pipeline(llm,vector_store):\n",
    "    # Creating ContextualCompressionRetriever\n",
    "    compressor = LLMChainExtractor.from_llm(llm)\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vector_store.as_retriever(search_type = \"mmr\")\n",
    "    )\n",
    "    return compression_retriever\n",
    "    # Contextual Compression will find the relevant records and only contains the relevant data from chunks instead of whole chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the LLM\n",
    "def initialize_llm():\n",
    "    llm = ChatGroq(\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Querying the retriever and LLM\n",
    "def query_llm(llm, retriever, query):\n",
    "    # Retrieve relevant documents\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "\n",
    "    # Capture the actual text chunks used\n",
    "    relevant_texts = [doc.page_content for doc in results]\n",
    "\n",
    "    # Use the LLM to process the retrieved documents\n",
    "    if results:\n",
    "        # Combine results for the LLM prompt, and track their sources\n",
    "        context = \"\\n\".join(relevant_texts)\n",
    "        prompt = f\"\"\"Use relevant information from 9th to 12th-grade textbooks to answer the student's query. If the context is helpful, incorporate it; otherwise, provide a general explanation. Avoid mentioning irrelevance and instead say, \"I cannot find relevant data from your book but I will explain the general concept.\" Encourage the student to ask follow-up questions related to the topics in the books.\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        Student Query:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Return both the response and relevant texts\n",
    "        return response, relevant_texts\n",
    "    else:\n",
    "        return \"No relevant documents found.\", []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='1.2.1 Definition\n",
      "A flowchart is a graphical presentation of the steps to solve\n",
      "Take shoes and socks\n",
      "Wear socks Wear shoes\n",
      "a problem. We use symbols for each step, and these symbols are connected with the help of arrows to show the flow of processing.\n",
      "Figure 1-6 shows a flowchart for the simple problem of wearing shoes with socks. It shows that not only the steps - | are important but also the order to complete a process. A Figure 1-6\n",
      "Sample flowchart\n",
      "Unit 1 — Problem Solving\n",
      "flowchart is used to visually communicate the steps in a process.' metadata={'chapter': 1, 'subtopic': 11}\n",
      "Creating new embeddings...\n"
     ]
    }
   ],
   "source": [
    "# Main execution flow\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your document\n",
    "    # documents = load_documents('resources/9thComputerScience_cleaned.txt')\n",
    "    with open('resources/9thComputerScience_cleaned.txt', 'r') as file:\n",
    "        documents = file.read()\n",
    "    # Split the text into smaller chunks\n",
    "    text_splitter = CustomTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    split_docs = text_splitter.split_text(documents)\n",
    "    print(split_docs[10])\n",
    "    embedded_docs = embed_documents(split_docs, embedding_model)\n",
    "    vector_store = store_embeddings(split_docs, embedding_model)\n",
    "    llm = initialize_llm()\n",
    "    retriever = build_rag_pipeline(llm,vector_store)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zubay\\AppData\\Local\\Temp\\ipykernel_4104\\3998235641.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  results = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Boolean proposition is a statement that can be either true or false. It's a sentence that has a clear and specific meaning, and its truth value can be determined. In other words, a Boolean proposition is a statement that can be classified as either true (T) or false (F).\n",
      "\n",
      "To be considered a Boolean proposition, a statement must meet two conditions:\n",
      "\n",
      "1. It must be a declarative sentence, meaning it must state something that can be verified as true or false.\n",
      "2. It must have a clear and specific meaning, leaving no room for ambiguity or uncertainty.\n",
      "\n",
      "Examples of Boolean propositions include:\n",
      "\n",
      "* \"Someone from our school can join the Pakistani Cricket Team.\" (This statement is either true or false.)\n",
      "* \"I will get an A+ grade in the board exam.\" (This statement is either true or false.)\n",
      "* \"This year's Pakistan Super League (PSL) final match will be played in Lahore.\" (This statement is either true or false.)\n",
      "\n",
      "On the other hand, sentences that are not Boolean propositions include:\n",
      "\n",
      "* Questions, such as \"How are you?\" or \"Is it hot outside?\"\n",
      "* Commands, such as \"Close the door.\"\n",
      "* Statements that are ambiguous or uncertain, such as \"I might go to the movies tonight.\"\n",
      "\n",
      "The concept of Boolean propositions is named after George Boole, who introduced the idea in his book \"The Laws of Thought.\" Boole's work laid the foundation for modern logic and computer science, and his ideas about Boolean propositions remain fundamental to these fields.\n",
      "\n",
      "Do you have any follow-up questions about Boolean propositions or would you like to explore other topics in logic or computer science?\n",
      "==============================================\n",
      "\n",
      "Relevant text chunks used in the response:\n",
      "Chunk: ==============================\n",
      "2.5.1 Boolean Proposition\n",
      "A proposition is a sentence that can either be true or false. For example, the following sentences are propositions.\n",
      "1. “Someone from our school can join Pakistani Cricket Team”\n",
      "2. “I will get A+ grade in board exam\"\n",
      "3. “I want to excel in mathematics”\n",
      "4. “This year Pakista\n"
     ]
    }
   ],
   "source": [
    "# # Example query \n",
    "query = \"explain boolean proposition\"\n",
    "response, relevant_texts = query_llm(llm, retriever, query)\n",
    "\n",
    "# # Output response and relevant text chunks\n",
    "print(response.content)\n",
    "print(\"==============================================\")\n",
    "print(\"\\nRelevant text chunks used in the response:\")\n",
    "for text in relevant_texts:\n",
    "    print(\"Chunk: ==============================\")\n",
    "    print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================\n",
      "2.5.1 Boolean Proposition\n",
      "A proposition is a sentence that can either be true or false. For example, the following sentences are propositions.\n",
      "1. “Someone from our school can join Pakistani Cricket Team”\n",
      "2. “I will get A+ grade in board exam\"\n",
      "3. “I want to excel in mathematics”\n",
      "4. “This year Pakistan Super League (PSL) final match will be played in Lahore”\n",
      "5. “I play chess”.\n",
      "But the following sentences are not propositions 1. How are you? 2. Close the door. 3. Is it hot outside?\n",
      "True and False are called Boolean values. The idea was given by George Boole (2 November 1815 —8 December 1864) in his book “The Laws of Thought”.\n"
     ]
    }
   ],
   "source": [
    "for text in relevant_texts:\n",
    "    print(\"=====================================\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query \n",
    "# query = \"who gave the idea of boolean values and on what date\"\n",
    "# response, relevant_texts = query_llm(llm, retriever, query)\n",
    "\n",
    "# # Output response and relevant text chunks\n",
    "# print(response.content)\n",
    "# print(\"==============================================\")\n",
    "# print(\"\\nRelevant text chunks used in the response:\")\n",
    "# for text in relevant_texts:\n",
    "#     print(\"Chunk: ==============================\")\n",
    "#     print(text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example query \n",
    "# query = \"how ip4 and ip6 works\"\n",
    "# response, relevant_texts = query_llm(llm, retriever, query)\n",
    "\n",
    "# # Output response and relevant text chunks\n",
    "# print(response.content)\n",
    "# print(\"==============================================\")\n",
    "# print(\"\\nRelevant text chunks used in the response:\")\n",
    "# for text in relevant_texts:\n",
    "#     print(\"Chunk: ==============================\")\n",
    "#     print(text[:300])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
